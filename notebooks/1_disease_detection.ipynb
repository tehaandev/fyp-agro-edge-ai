{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72867843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "PROJECT_DIR = \"..\"\n",
    "DATA_DIR = f\"{PROJECT_DIR}/data/processed/PlantVillage_Binary\"\n",
    "LOGS_DIR = f\"{PROJECT_DIR}/logs/disease_detection\"\n",
    "\n",
    "# Finetuning parameters\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIDE_LENGTH = 128\n",
    "IMG_SIZE = (IMG_SIDE_LENGTH, IMG_SIDE_LENGTH)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BUFFER_SIZE = 2\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "LABEL_MODE = 'binary'\n",
    "\n",
    "CLASS_WEIGHTS = None # Not required because both classes have equal images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7499d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU memory growth set to True\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=f\"{DATA_DIR}/train\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=LABEL_MODE,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f\"{DATA_DIR}/val\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=LABEL_MODE,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f\"{DATA_DIR}/test\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=LABEL_MODE,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d121f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Class 0 maps to:\", class_names[0])\n",
    "print(\"Class 1 maps to:\", class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"), \n",
    "    layers.RandomRotation(0.3),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomBrightness(0.3),\n",
    "    layers.RandomContrast(0.3),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    # Add noise\n",
    "    layers.GaussianNoise(0.1),     \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be524338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the augmented data with labels\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get a batch of data\n",
    "for images, labels in train_ds.take(1):\n",
    "    # Take first 8 images from the batch\n",
    "    for i in range(8):\n",
    "        # Original image\n",
    "        plt.subplot(4, 4, i*2 + 1)\n",
    "        # Denormalize for display (since normalization_layer is applied in model)\n",
    "        original_img = images[i].numpy().astype(\"uint8\")\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(f'Original: {class_names[int(labels[i])]}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Augmented image\n",
    "        plt.subplot(4, 4, i*2 + 2)\n",
    "        # Apply data augmentation\n",
    "        augmented_img = data_augmentation(tf.expand_dims(images[i], 0), training=True)\n",
    "        augmented_img = tf.cast(augmented_img[0], tf.uint8).numpy()\n",
    "        plt.imshow(augmented_img)\n",
    "        plt.title(f'Augmented: {class_names[int(labels[i])]}')\n",
    "        plt.axis('off')\n",
    "    break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show distribution of classes in the current batch\n",
    "unique, counts = np.unique(labels.numpy(), return_counts=True)\n",
    "print(f\"\\nClass distribution in this batch:\")\n",
    "for class_idx, count in zip(unique, counts):\n",
    "    print(f\"{class_names[int(class_idx)]}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abe87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(1000).prefetch(buffer_size=BUFFER_SIZE)\n",
    "val_ds = val_ds.prefetch(buffer_size=BUFFER_SIZE)\n",
    "test_ds = test_ds.prefetch(buffer_size=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f17db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIDE_LENGTH, IMG_SIDE_LENGTH, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False  # freeze base\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    normalization_layer,\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(IMG_SIDE_LENGTH, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b79ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=LR),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for better training control\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(f\"{PROJECT_DIR}/models\", exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "training_start_time = time.time()\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59756c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End timing\n",
    "training_end_time = time.time()\n",
    "print(f\"Training completed in {training_end_time - training_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d85a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal improvements to existing plots\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='train accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy', linewidth=2)\n",
    "plt.title('Accuracy', fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='train loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='val loss', linewidth=2)\n",
    "plt.title('Loss', fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['precision'], label='train precision', linewidth=2)\n",
    "plt.plot(history.history['val_precision'], label='val precision', linewidth=2)\n",
    "plt.title('Precision', fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['recall'], label='train recall', linewidth=2)\n",
    "plt.plot(history.history['val_recall'], label='val recall', linewidth=2)\n",
    "plt.title('Recall', fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_results = model.evaluate(test_ds)\n",
    "test_loss = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "test_precision = test_results[2] if len(test_results) > 2 else None\n",
    "test_recall = test_results[3] if len(test_results) > 3 else None\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "if test_precision is not None:\n",
    "    print(f\"Test Precision: {test_precision}\")\n",
    "if test_recall is not None:\n",
    "    print(f\"Test Recall: {test_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your custom image\n",
    "from keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Path to your custom image\n",
    "custom_image_path_healthy = f\"../data/custom/image.png\" \n",
    "custom_image_path_diseased = f\"../data/custom/bp_diseased.jpg\" \n",
    "\n",
    "diseased_image = load_img(custom_image_path_diseased, target_size=IMG_SIZE)\n",
    "diseased_image_array = img_to_array(diseased_image)\n",
    "diseased_image_array = np.expand_dims(diseased_image_array, axis=0)\n",
    "diseased_image_array = diseased_image_array / 255.0\n",
    "\n",
    "healthy_image = load_img(custom_image_path_healthy, target_size=IMG_SIZE)\n",
    "healthy_image_array = img_to_array(healthy_image)\n",
    "healthy_image_array = np.expand_dims(healthy_image_array, axis=0)\n",
    "healthy_image_array = healthy_image_array / 255.0\n",
    "\n",
    "healthy_predictions = model.predict(healthy_image_array)\n",
    "diseased_predictions = model.predict(diseased_image_array)\n",
    "\n",
    "print(\"Raw predictions:\")\n",
    "print(f\"Healthy image raw score: {healthy_predictions[0][0]:.4f}\")\n",
    "print(f\"Diseased image raw score: {diseased_predictions[0][0]:.4f}\")\n",
    "print()\n",
    "\n",
    "predicted_healthy_class = (healthy_predictions[0][0] > 0.1).astype(int)\n",
    "predicted_diseased_class = (diseased_predictions[0][0] > 0.1).astype(int)\n",
    "\n",
    "print(f\"Healthy Image Prediction: {class_names[predicted_healthy_class]} (Score: {healthy_predictions[0][0]:.4f})\")\n",
    "print(f\"Diseased Image Prediction: {class_names[predicted_diseased_class]} (Score: {diseased_predictions[0][0]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bf91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for test set\n",
    "y_pred_probs = model.predict(test_ds)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Get true labels\n",
    "y_true = []\n",
    "for _, labels in test_ds:\n",
    "    y_true.extend(labels.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Run Logger\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Calculate training time (assuming you stored start time before training)\n",
    "# If you didn't store start time, you can estimate from history length\n",
    "try:\n",
    "    training_time_seconds = training_end_time - training_start_time\n",
    "except NameError:\n",
    "    # Estimate based on epochs (rough approximation)\n",
    "    epochs_completed = len(history.history['accuracy'])\n",
    "    estimated_time_per_epoch = 60  # seconds (adjust based on your setup)\n",
    "    training_time_seconds = epochs_completed * estimated_time_per_epoch\n",
    "\n",
    "training_time_minutes = training_time_seconds / 60\n",
    "training_time_hours = training_time_minutes / 60\n",
    "\n",
    "# Prepare training log data\n",
    "training_log = {\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"hyperparameters\": {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"img_side_length\": IMG_SIDE_LENGTH,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LR,\n",
    "        \"buffer_size\": BUFFER_SIZE,\n",
    "        \"label_mode\": LABEL_MODE,\n",
    "        \"class_weights\": CLASS_WEIGHTS,\n",
    "        \"data_augmentation\": \"RandomFlip(horizontal) + RandomRotation(0.5)\",\n",
    "        \"base_model\": \"MobileNetV2\",\n",
    "        \"base_model_trainable\": False,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"dropout_rate\": 0.3,\n",
    "        \"dense_layer_units\": IMG_SIDE_LENGTH\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"data_dir\": DATA_DIR,\n",
    "        \"class_names\": class_names,\n",
    "        \"total_classes\": len(class_names)\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"early_stopping\": {\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"patience\": 5,\n",
    "            \"restore_best_weights\": True\n",
    "        },\n",
    "        \"reduce_lr_on_plateau\": {\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 3,\n",
    "            \"min_lr\": 1e-7\n",
    "        }\n",
    "    },\n",
    "    \"training_time\": {\n",
    "        \"total_seconds\": float(training_time_seconds),\n",
    "        \"total_minutes\": float(training_time_minutes),\n",
    "        \"total_hours\": float(training_time_hours),\n",
    "        \"formatted\": f\"{int(training_time_hours):02d}h {int(training_time_minutes % 60):02d}m {int(training_time_seconds % 60):02d}s\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"final_train_accuracy\": float(history.history['accuracy'][-1]),\n",
    "        \"final_val_accuracy\": float(history.history['val_accuracy'][-1]),\n",
    "        \"final_train_loss\": float(history.history['loss'][-1]),\n",
    "        \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "        \"best_val_accuracy\": float(max(history.history['val_accuracy'])),\n",
    "        \"best_val_loss\": float(min(history.history['val_loss'])),\n",
    "        \"epochs_trained\": len(history.history['accuracy']),\n",
    "        \"test_accuracy\": float(test_accuracy),\n",
    "        \"test_loss\": float(test_loss),\n",
    "        \"test_f1_score\": float(f1),\n",
    "        \"test_precision\": float(precision),\n",
    "        \"test_recall\": float(recall)    \n",
    "    },\n",
    "    \"training_history\": {\n",
    "        \"accuracy\": [float(x) for x in history.history['accuracy']],\n",
    "        \"val_accuracy\": [float(x) for x in history.history['val_accuracy']],\n",
    "        \"loss\": [float(x) for x in history.history['loss']],\n",
    "        \"val_loss\": [float(x) for x in history.history['val_loss']]\n",
    "    },\n",
    "    \"custom_predictions\": {\n",
    "        \"healthy_image_score\": float(healthy_predictions[0][0]),\n",
    "        \"diseased_image_score\": float(diseased_predictions[0][0]),\n",
    "        \"healthy_image_prediction\": class_names[predicted_healthy_class],\n",
    "        \"diseased_image_prediction\": class_names[predicted_diseased_class]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON file with timestamp\n",
    "timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"{LOGS_DIR}/training_log_{timestamp_str}.json\"\n",
    "\n",
    "with open(log_filename, 'w') as f:\n",
    "    json.dump(training_log, f, indent=2)\n",
    "\n",
    "print(f\"Training log saved to: {log_filename}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== TRAINING SUMMARY ===\")\n",
    "print(f\"Model: MobileNetV2 (frozen) + Dense({IMG_SIDE_LENGTH}) + Dense(1)\")\n",
    "print(f\"Training Time: {training_log['training_time']['formatted']}\")\n",
    "print(f\"Final Training Accuracy: {training_log['results']['final_train_accuracy']:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {training_log['results']['final_val_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {training_log['results']['test_accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score: {training_log['results']['test_f1_score']:.4f}\")\n",
    "print(f\"Epochs Trained: {training_log['results']['epochs_trained']}\")\n",
    "print(f\"Best Validation Accuracy: {training_log['results']['best_val_accuracy']:.4f}\")\n",
    "print(f\"Learning Rate: {LR}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(f\"{PROJECT_DIR}/models/plant_disease__binary_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model successfully converted to TFLite format!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
